{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tasks with Kinematic Time Series from Head Pose Estimation from HMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Time Series Classification\n",
    "\n",
    "Check `01_...ipynb` to see details of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # For .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__))) #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "except: # For .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "print(\"File Path:\", this_path)\n",
    "sys.path.append(os.path.join(this_path, \"kinemats\"))\n",
    "\n",
    "# Import classes\n",
    "import time\n",
    "import utils  # Utils for generation of files and paths\n",
    "import quaternion_math\n",
    "\n",
    "from plotter.ts_visualization import *\n",
    "import ts_processing\n",
    "import ts_classification\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import sktime.utils.data_processing\n",
    "# from sktime.classification.shapelet_based import MrSEQLClassifier\n",
    "# from sktime.classification.interval_based import SupervisedTimeSeriesForest\n",
    "# from sktime.classification.dictionary_based import TemporalDictionaryEnsemble\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sktime.classification.shapelet_based import ROCKETClassifier\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sklearn.linear_model import SGDClassifier  # Used to train the transformed features from MiniRocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "import experiment_config\n",
    "from experiment_config import Datasets, DataRepresentation, Classifiers\n",
    "\n",
    "### SPECIFIC CONSTANTS\n",
    "\n",
    "# All the files generated from this notebook are in a subfolder with this name\n",
    "NOTEBOOK_SUBFOLDER_NAME = '3_TimeSeriesClassifiers/'\n",
    "\n",
    "# Filenames of created files from this script\n",
    "FILENAME_DATASET_QUATERNION = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Quaternion))      # generates \"dataset_quaternion\"\n",
    "FILENAME_DATASET_EULER = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Euler))\n",
    "FILENAME_DATASET_YAW = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Yaw))\n",
    "\n",
    "#### NOTE: This dictionary is reassigned later in the code whenever the datasets are generated.\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  None,\n",
    "    DataRepresentation.Euler:       None,\n",
    "    DataRepresentation.Yaw:         None,\n",
    "    DataRepresentation.All:         None,\n",
    "}\n",
    "# Dictionary to convert a datarepresentation into a num - To be stored in the numpy array for results\n",
    "DICT_DATA_TO_NUM = { k:i for i,k in enumerate(DICT_DATA.keys())}\n",
    "\n",
    "# Combination of each dimension with each transformation\n",
    "HEADER_QUAT = [\"qw\",\"qi\",\"qj\",\"qk\"]\n",
    "HEADER_EULER = [\"yaw\",\"pitch\",\"roll\"]\n",
    "HEADER_YAW = [\"yaw\"]\n",
    "HEADER_ALL = HEADER_QUAT + HEADER_EULER\n",
    "print(f\"Example header {HEADER_ALL}\")\n",
    "\n",
    "\n",
    "#### Classification methods to apply.\n",
    "DICT_CLASSIFIERS = {\n",
    "    Classifiers.KNN:    KNeighborsTimeSeriesClassifier(n_neighbors=experiment_config.KNN_TS_N_NEIGH,\n",
    "                                                        distance=\"dtw\",\n",
    "                                                        distance_params={\"w\":experiment_config.KNN_TS_DTW_WARPING_WINDOW}),\n",
    "    # Classifiers.MrSEQL: MrSEQLClassifier(),\n",
    "    # Classifiers.TDE: TemporalDictionaryEnsemble(time_limit=experiment_config.TDE_MAX_TIME,\n",
    "                                #  max_ensemble_size=experiment_config.TDE_MAX_ENSEMBLE_SIZE,\n",
    "                                #  randomly_selected_params=experiment_config.TDE_MAX_SELECTED_PARAMS,\n",
    "                                #  random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    ### Classifiers.STSF: SupervisedTimeSeriesForest(n_estimators=experiment_config.STSF_N_ESTIMATORS, \\\n",
    "                            ### n_jobs=experiment_config.N_JOBS_PARALLEL, \\\n",
    "                            ### random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    Classifiers.ROCKET: ROCKETClassifier(num_kernels=experiment_config.ROCKET_N_KERNELS,\n",
    "                            n_jobs=experiment_config.N_JOBS_PARALLEL,\n",
    "                            random_state=experiment_config.MC_RANDOM_SEED), \n",
    "    Classifiers.MiniRocket: MiniRocketMultivariate(num_features=experiment_config.MINIROCKET_N_KERNELS,\n",
    "                            max_dilations_per_kernel=experiment_config.MINIROCKET_MAX_DILATIONS,\n",
    "                            random_state=experiment_config.MC_RANDOM_SEED)\n",
    "}\n",
    "\n",
    "# Linear model to be trained after MiniRocket transformer.\n",
    "MINIROCKET_LINEAR_MODEL = SGDClassifier(loss=\"log\", n_jobs=experiment_config.N_JOBS_PARALLEL, random_state=experiment_config.MC_RANDOM_SEED)\n",
    "\n",
    "## K-Fold partition\n",
    "N_SPLITS_CV = experiment_config.CV_NUM_FOLDS # Number of folds for Cross-validation\n",
    "strat_KFold = StratifiedKFold(n_splits=N_SPLITS_CV,\n",
    "                              random_state=experiment_config.MC_RANDOM_SEED,\n",
    "                              shuffle=True)\n",
    "\n",
    "# Scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "Generate paths to write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STR_DATASET = str(experiment_config.DATASET_MAIN)+\"/\"\n",
    "def gen_path_plot(filename):\n",
    "    # Generates full paths for PLOTS just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.PLOT_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME, \\\n",
    "                                        file_extension=experiment_config.IMG_FORMAT, save_files=experiment_config.EXPORT_PLOTS)\n",
    "\n",
    "def gen_path_temp(filename, subfolders=\"\", extension=experiment_config.TEMP_FORMAT):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.TEMP_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)\n",
    "\n",
    "def gen_path_results(filename, subfolders=\"\", extension=\"\"):\n",
    "    # Generates full paths for RESULTS FILES (like pandas dataframes)\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.RESULTS_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS: Load and preprocess\n",
    "\n",
    "If the files do not exist. Generate them by running `01_...ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t>>>LOADING DATASETS\")\n",
    "dataset = None\n",
    "classes = None\n",
    "\n",
    "# Coordinate reference system. All datasets should be transformed to match this coordinate system.\n",
    "AXIS_INSTANCE=0\n",
    "AXIS_TIME=1\n",
    "AXIS_DIMENSIONS=2"
   ]
  },
  {
   "source": [
    "# Load previously processed datasets\n",
    "dataset_quaternion =  utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_QUATERNION ) )\n",
    "dataset_euler = utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_EULER ) )\n",
    "dataset_yaw = utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_YAW ) )\n",
    "\n",
    "# Concatenation of quaternion and Euler\n",
    "dataset_all = np.concatenate([dataset_quaternion, dataset_euler], axis=AXIS_DIMENSIONS)\n",
    "\n",
    "# Variable used to calculate general stats\n",
    "dataset = dataset_quaternion"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.IMT:\n",
    "    \n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    labels = pd.read_csv(labels_filename)\n",
    "    timestamps = np.loadtxt(timestamps_filename)\n",
    "\n",
    "    # # Classes are the labels of the videos \n",
    "    # classes = labels[\"videoId\"].to_numpy(dtype=np.int32)\n",
    "    # # Classes are the user watching the videos\n",
    "    #classes = labels[\"user\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    classes = labels[experiment_config.CLASS_COLUMN_NAME].to_numpy(dtype=np.int32)\n",
    "    \n",
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "        \n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    labels = pd.read_csv(labels_filename)\n",
    "    timestamps = np.loadtxt(timestamps_filename)\n",
    "    \n",
    "    # # Classes are the labels of the videos \n",
    "    # classes = labels[\"videoId\"].to_numpy(dtype=np.int32)\n",
    "    # # Classes are the user watching the videos\n",
    "    #classes = labels[\"user\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    classes = labels[experiment_config.CLASS_COLUMN_NAME].to_numpy(dtype=np.int32)"
   ]
  },
  {
   "source": [
    "## Summary dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = np.unique(classes).size\n",
    "\n",
    "if(dataset.ndim == 2):\n",
    "    dataset = np.expand_dims(dataset, axis=2)\n",
    "\n",
    "num_ts = dataset.shape[0]\n",
    "length_ts = dataset.shape[1]\n",
    "num_dims = dataset.shape[2]\n",
    "\n",
    "print(\"Timestamps:\", type(timestamps), timestamps.shape)\n",
    "print(\"Dataset\", type(dataset), dataset.shape)\n",
    "print(\"Classes\", type(classes), classes.shape, )\n",
    "print(f\"num_classes={num_classes}\")\n",
    "print(f\"num_ts={num_ts}\")\n",
    "print(f\"length_ts={length_ts}\")\n",
    "print(f\"num_dims={num_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Summary`\n",
    "\n",
    "Until this point, the head movements are stored as in these data representations: \n",
    "- Quaternion (`dataset_quaternion`)\n",
    "- Euler Angles (`dataset_euler`)\n",
    "- Spherical Angles (`dataset_spherical`)\n",
    "- Rotation around Z-axis (`dataset_yaw`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE:` REDEFINITION OF DATASETS\n",
    "The dictionary is redefined now, since the datasets were properly loaded/created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into sktime compatible\n",
    "df_quaternion   = sktime.utils.data_processing.from_3d_numpy_to_nested(np.swapaxes(dataset_quaternion, AXIS_TIME, AXIS_DIMENSIONS), column_names=HEADER_QUAT)\n",
    "df_euler        = sktime.utils.data_processing.from_3d_numpy_to_nested(np.swapaxes(dataset_euler, AXIS_TIME, AXIS_DIMENSIONS), column_names=HEADER_EULER)\n",
    "df_yaw          = sktime.utils.data_processing.from_3d_numpy_to_nested(np.swapaxes(dataset_yaw, AXIS_TIME, AXIS_DIMENSIONS), column_names=HEADER_YAW)\n",
    "df_all          = sktime.utils.data_processing.from_3d_numpy_to_nested(np.swapaxes(dataset_all, AXIS_TIME, AXIS_DIMENSIONS), column_names=HEADER_ALL)\n",
    "\n",
    "# REDEFINE DICT WITH CORRESPONDING DATASETS\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  df_quaternion,\n",
    "    DataRepresentation.Euler:       df_euler,\n",
    "    DataRepresentation.Yaw:         df_yaw,\n",
    "    DataRepresentation.All:         df_all,\n",
    "}\n",
    "\n",
    "# Class labels\n",
    "y = classes\n",
    "# X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_quaternion, y, random_state=experiment_config.MC_RANDOM_SEED)\n",
    "\n",
    "# clf = MrSEQLClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "# clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# STATE-OF-THE-ART (SOTA) TIME-SERIES CLASSIFIERS\n",
    "\n",
    "- Mr-SEQL\n",
    "- STSF: Univariate `Not used`\n",
    "- TDE\n",
    "- ROCKET\n",
    "- MiniRocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter_datarep = DataRepresentation.Euler # Edit here!\n",
    "# iter_data = DICT_DATA[iter_datarep]\n",
    "# iter_classifier = Classifiers.ROCKET # Edit here!\n",
    "# iter_model = DICT_CLASSIFIERS[iter_classifier]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Test CV iterator\n",
    "\n",
    "# # The key strings are based on what the cross_validate() function generates automatically in 02_FeatureBasedClass.ipynb\n",
    "# cv_results = {\n",
    "#     \"fit_time\": [],\n",
    "#     \"score_time\": [],\n",
    "#     \"test_accuracy\": [],\n",
    "#     \"test_precision_macro\": [],\n",
    "#     \"test_recall_macro\": [],\n",
    "#     \"test_f1_macro\": []\n",
    "# }\n",
    "\n",
    "# for f, (train_idx, test_idx) in enumerate(strat_KFold.split(iter_data, y)):\n",
    "#     print(f\"Fold: {f} > \\t train,test > \\t{train_idx.shape, test_idx.shape}\")\n",
    "#     # if(f==N_SPLITS_CV-1):\n",
    "#         # print(f\"{train_idx,test_idx}\")\n",
    "\n",
    "#     # Separate training and testing sets\n",
    "#     X_train = iter_data.iloc[train_idx,:]\n",
    "#     X_test = iter_data.iloc[test_idx,:]\n",
    "#     y_train = y[train_idx]\n",
    "#     y_test = y[test_idx]\n",
    "\n",
    "#     # Fitting\n",
    "#     t0 = time.time()\n",
    "#     iter_model.fit(X_train,y_train) # Train model!\n",
    "#     t1 = (time.time() - t0)\n",
    "#     cv_results[\"fit_time\"].append(t1)\n",
    "\n",
    "#     # Predicting\n",
    "#     t0 = time.time()\n",
    "#     y_pred = iter_model.predict(X_test) # Test model!\n",
    "#     t1 = (time.time() - t0)\n",
    "#     cv_results[\"score_time\"].append(t1)\n",
    "\n",
    "#     # Scores\n",
    "#     cv_results[\"test_accuracy\"].append(         accuracy_score(y_test, y_pred) )\n",
    "#     cv_results[\"test_precision_macro\"].append(  precision_score(y_test, y_pred, average=\"macro\") )\n",
    "#     cv_results[\"test_recall_macro\"].append(     recall_score(y_test, y_pred, average=\"macro\") )\n",
    "#     cv_results[\"test_f1_macro\"].append(         f1_score(y_test, y_pred, average=\"macro\") )\n",
    "\n",
    "# # Add more information to CV results\n",
    "# cv_results[experiment_config.COLUMNS_LABELS[0]] = [ str(iter_datarep) ] * N_SPLITS_CV # Datarep\n",
    "# cv_results[experiment_config.COLUMNS_LABELS[1]] = [ str(iter_classifier) ] * N_SPLITS_CV # Classifier\n",
    "# cv_results[experiment_config.COLUMNS_LABELS[2]] = np.arange(N_SPLITS_CV) # Fold\n",
    "# # for k,v in cv_results.items():\n",
    "#     # print(f\"{k}: \\t{v}\")\n",
    "\n",
    "# # From dict to pandas DataFrame\n",
    "# iteration_results = pd.DataFrame(cv_results.copy(), columns=sorted(cv_results.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the dataframe with compiled results\n",
    "classif_results = None"
   ]
  },
  {
   "source": [
    "## Time-series Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\t>>>LOADING/CREATING CLASSIFICATION RESULTS\")\n",
    "# Filename of the file containing demographics and HMD movements data\n",
    "classification_results_filename = gen_path_results(experiment_config.RESULTS_FILENAME, extension=\".csv\")\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [classification_results_filename]\n",
    "\n",
    "RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        classif_results = pd.read_csv(input_files[0]) # pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        idx = 1 # Iterator for row index\n",
    "        total_iter = len(DICT_DATA.keys()) * len(DICT_CLASSIFIERS.keys()) # How many total iterations are going to be conducted\n",
    "        print(f\"Iterating {total_iter} times\")\n",
    "        # f = IntProgress(min=0, max=classif_results.shape[0])\n",
    "        # display(f)\n",
    "\n",
    "        ### Iterate over datasets and classifiers\n",
    "\n",
    "        # For each dataset\n",
    "        for iter_datarep, iter_data in DICT_DATA.items():\n",
    "            # Apply each classifier model\n",
    "            for iter_classifier,iter_model in DICT_CLASSIFIERS.items():\n",
    "                \n",
    "                # Print messages\n",
    "                if idx%experiment_config.DISPLAY_ITER_STEP==0: \n",
    "                    print(f\" | Iteration {idx}/{total_iter} \\t> dataset: {str(iter_datarep)} \\tclassifier: {str(iter_classifier)}\")\n",
    "                idx = idx + 1\n",
    "                \n",
    "                ### Cross validation\n",
    "                # The key strings are based on what the cross_validate() function generates automatically in 02_FeatureBasedClass.ipynb\n",
    "                cv_results = {\n",
    "                    \"fit_time\": [],\n",
    "                    \"score_time\": [],\n",
    "                    \"test_accuracy\": [],\n",
    "                    \"test_precision_macro\": [],\n",
    "                    \"test_recall_macro\": [],\n",
    "                    \"test_f1_macro\": []\n",
    "                }\n",
    "\n",
    "                for f, (train_idx, test_idx) in enumerate(strat_KFold.split(iter_data, y)):\n",
    "                    print(f\"Fold: {f} > \\t train,test > \\t{train_idx.shape, test_idx.shape}\")\n",
    "                    # if(f==N_SPLITS_CV-1):\n",
    "                        # print(f\"{train_idx,test_idx}\")\n",
    "\n",
    "                    # Separate training and testing sets\n",
    "                    X_train = iter_data.iloc[train_idx,:]\n",
    "                    X_test = iter_data.iloc[test_idx,:]\n",
    "                    y_train = y[train_idx]\n",
    "                    y_test = y[test_idx]\n",
    "\n",
    "                    \n",
    "                    #### Fitting\n",
    "                    t0 = time.time()        # Start measuring fitting time\n",
    "\n",
    "                    ## NOTE! MiniRocket transforms the time series using random convolutional kernels, and then trains a LINEAR CLASSIFIER.\n",
    "                    #       As suggested in the paper, for >10k training samples, it is recommended to use a Logistic Regression trained\n",
    "                    #       using stochastic gradient descent\n",
    "                    if (iter_classifier is Classifiers.MiniRocket):\n",
    "                        # At the beginning\n",
    "                        iter_model.fit(X_train) # `iter_model` contains just the transformer\n",
    "                        X_train = iter_model.transform(X_train)     # Replace the time series with the transformer\n",
    "                        # Fit model\n",
    "                        MINIROCKET_LINEAR_MODEL.fit(X_train, y_train)   # Fit linear model\n",
    "                    else:\n",
    "                        # Fit model\n",
    "                        iter_model.fit(X_train,y_train) # Train model!\n",
    "\n",
    "                    t1 = (time.time() - t0)         # End measuring fitting time\n",
    "                    cv_results[\"fit_time\"].append(t1)\n",
    "\n",
    "                    #### Predicting\n",
    "                    t0 = time.time()\n",
    "                    if (iter_classifier is Classifiers.MiniRocket):\n",
    "                        # Predict model\n",
    "                        X_test = iter_model.transform(X_test)       # Transform test time-series\n",
    "                        y_pred = MINIROCKET_LINEAR_MODEL.predict(X_test) # Test model!\n",
    "                    else:\n",
    "                        # Predict model\n",
    "                        y_pred = iter_model.predict(X_test) # Test model!\n",
    "                    t1 = (time.time() - t0)\n",
    "                    cv_results[\"score_time\"].append(t1)\n",
    "\n",
    "                    #### Scoring\n",
    "                    cv_results[\"test_accuracy\"].append(         accuracy_score(y_test, y_pred) )\n",
    "                    cv_results[\"test_precision_macro\"].append(  precision_score(y_test, y_pred, average=\"macro\") )\n",
    "                    cv_results[\"test_recall_macro\"].append(     recall_score(y_test, y_pred, average=\"macro\") )\n",
    "                    cv_results[\"test_f1_macro\"].append(         f1_score(y_test, y_pred, average=\"macro\") )\n",
    "                # } END: Cross-validation\n",
    "\n",
    "                # Add more information to CV results\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[0]] = [ str(iter_datarep) ] * N_SPLITS_CV # Datarep\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[1]] = [ str(iter_classifier) ] * N_SPLITS_CV # Classifier\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[2]] = np.arange(N_SPLITS_CV) # Fold\n",
    "                # for k,v in cv_results.items():\n",
    "                    # print(f\"{k}: \\t{v}\")\n",
    "\n",
    "                # From dict to pandas DataFrame\n",
    "                iteration_results = pd.DataFrame(cv_results.copy(), columns=sorted(cv_results.keys()))\n",
    "\n",
    "                ### END: CROSS VALIDATION\n",
    "                \n",
    "                # Add more information to CV results\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[0]] = [ str(iter_datarep) ] * N_SPLITS_CV # Datarep\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[1]] = [ str(iter_classifier) ] * N_SPLITS_CV # Classifier\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[2]] = np.arange(N_SPLITS_CV) # Fold\n",
    "                # for k,v in cv_results.items():\n",
    "                    # print(f\"{k}: \\t{v}\")\n",
    "\n",
    "                # From dict to pandas DataFrame\n",
    "                iteration_results = pd.DataFrame(cv_results.copy(), columns=sorted(cv_results.keys()))\n",
    "\n",
    "                # Extend dataframe\n",
    "                if classif_results is None:\n",
    "                    classif_results = iteration_results\n",
    "                else:\n",
    "                    classif_results = classif_results.append(iteration_results, ignore_index=True)\n",
    "\n",
    "                # END: Classifiers\n",
    "\n",
    "            # } END: Dataset\n",
    "            print(f\"\\t\\t>> FINISHED (dataset)\")\n",
    "        # } END: Loop done\n",
    "\n",
    "        # for i in [0,1,2,3]:\n",
    "            # classif_results[experiment_config.COLUMNS_LABELS[i]] = classif_results[experiment_config.COLUMNS_LABELS[i]].astype(int)\n",
    "\n",
    "        # Save files\n",
    "        classif_results.to_csv(input_files[0], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "source": [
    "## PLOT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Data preprocessing for plotting\n",
    "# # Take only accuracy for class1 over all runs and delete irrelevant columns\n",
    "# plotsdataset = classif_results[( classif_results['classLabel']==1 )] #& (classif_results['distMetric'] == DICT_DISTMETRIC_TO_NUM[metric_txt]) )]   # Filter for plotting\n",
    "# plotsdataset.drop(['distMetric','mcIter','classLabel','precision','recall'], axis=1, inplace=True)\n",
    "# plotsdataset.tail()\n",
    "\n",
    "# ## Plot\n",
    "# if(experiment_config.SHOW_PLOTS): plot_violin_mc(plotsdataset, x_colname=\"dataRep\", y_colname=\"accuracy\", hue_colname=\"classifier\",\\\n",
    "#                 suptitle=f'Classification accuracy over {MC_ITERATIONS} Monte-Carlo simulations',\\\n",
    "#                 title = f\"State of the Art - Classifiers\",\\\n",
    "#                 x_ticklabels = experiment_config.HEADERS_DATASETS, \\\n",
    "#                 y_lim=[0,1], \\\n",
    "#                 n_rows=1, n_cols=1, figsize=(8,6), \\\n",
    "#                 save_path=gen_path_plot(f\"accuracies_SoA\"),\n",
    "#                 boxplot_instead_violin=True)"
   ]
  },
  {
   "source": [
    "## EOF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd0528e8b0b812fc245fe358bb9a8e3f7cc22b8dcd316088c7aabdbb253c130bd9d",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "f3aec1f4fef7a88c2258d5b84a8b82909f076cff2bcb16988c856ebc42b66954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}