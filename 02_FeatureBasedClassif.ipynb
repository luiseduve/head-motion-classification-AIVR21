{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tasks with Kinematic Time Series from Head Pose Estimation from HMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Time Series Classification\n",
    "\n",
    "Check `01_...ipynb` to see details of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File Path: d:\\dsv\\dev\\git_repos\\headmov-classif-360videos/\n"
     ]
    }
   ],
   "source": [
    "# Add files to sys.path\n",
    "from pathlib import Path\n",
    "import sys,os\n",
    "this_path = None\n",
    "try:    # For .py\n",
    "    this_path = str(os.path.dirname(os.path.abspath(__file__))) #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "except: # For .ipynb\n",
    "    this_path = str(Path().absolute())+\"/\" #str(Path().absolute())+\"/\" # str(os.path.dirname(__file__))\n",
    "print(\"File Path:\", this_path)\n",
    "sys.path.append(os.path.join(this_path, \"kinemats\"))\n",
    "\n",
    "# Import classes\n",
    "import utils  # Utils for generation of files and paths\n",
    "import quaternion_math\n",
    "\n",
    "from plotter.ts_visualization import *\n",
    "import ts_processing\n",
    "import ts_classification\n",
    "\n",
    "# Import data science libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature based classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Example header ['qw_raw', 'qi_raw', 'qj_raw', 'qk_raw', 'qw_vel', 'qi_vel', 'qj_vel', 'qk_vel', 'qw_acc', 'qi_acc', 'qj_acc', 'qk_acc']\n"
     ]
    }
   ],
   "source": [
    "# CONSTANTS\n",
    "import experiment_config\n",
    "from experiment_config import Datasets, DataRepresentation, Classifiers\n",
    "from ts_classification import EnumDistMetrics\n",
    "\n",
    "# All the files generated from this notebook are in a subfolder with this name\n",
    "NOTEBOOK_SUBFOLDER_NAME = '2_FeatureBasedClassifiers/'\n",
    "\n",
    "# Filenames of created files from this script\n",
    "FILENAME_DATASET_QUATERNION = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Quaternion))      # generates \"dataset_quaternion\"\n",
    "FILENAME_DATASET_EULER = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Euler))\n",
    "FILENAME_DATASET_YAW = str(experiment_config.PREFIX_DATASET+str(DataRepresentation.Yaw))\n",
    "\n",
    "#### NOTE: This dictionary is reassigned later in the code whenever the datasets are generated.\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  None,\n",
    "    DataRepresentation.Euler:       None,\n",
    "    DataRepresentation.Yaw:         None,\n",
    "    DataRepresentation.All:         None,\n",
    "}\n",
    "# Dictionary to convert a datarepresentation into a num - To be stored in the numpy array for results\n",
    "DICT_DATA_TO_NUM = { k:i for i,k in enumerate(DICT_DATA.keys())}\n",
    "\n",
    "### Headers of tabular data for feature-based classifiers\n",
    "SUFFIX_DIFFERENTIAL_TRANSFORMATIONS = [\"raw\",\"vel\",\"acc\"]\n",
    "\n",
    "# Combination of each dimension with each transformation\n",
    "HEADER_QUAT = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"qw\",\"qi\",\"qj\",\"qk\"] ]\n",
    "HEADER_EULER = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"yaw\",\"pitch\",\"roll\"] ]\n",
    "HEADER_YAW = [ h+\"_\"+s for s in SUFFIX_DIFFERENTIAL_TRANSFORMATIONS for h in [\"yaw\"] ]\n",
    "HEADER_ALL = HEADER_QUAT + HEADER_EULER\n",
    "print(f\"Example header {HEADER_QUAT}\")\n",
    "\n",
    "# Setup of overlapping windows to extract features from time series\n",
    "SLIDING_WINDOW_WIDTH_SECS = 1\n",
    "SLIDING_WINDOW_OVERLAP_SECS = 0 # Not used!!\n",
    "\n",
    "#### Classification methods to apply.\n",
    "DICT_CLASSIFIERS = {\n",
    "    Classifiers.KNN:    KNeighborsClassifier(n_neighbors=experiment_config.KNN_N_NEIGH),\n",
    "    Classifiers.DT:     DecisionTreeClassifier(max_depth=experiment_config.DT_MAX_DEPTH, criterion='entropy', random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    Classifiers.RF:     RandomForestClassifier(n_estimators=experiment_config.RF_N_ESTIMATORS, max_depth=experiment_config.RF_MAX_DEPTH, criterion='entropy', random_state=experiment_config.MC_RANDOM_SEED),\n",
    "    Classifiers.GBM:    GradientBoostingClassifier(n_estimators=experiment_config.GBM_N_ESTIMATORS, max_depth=experiment_config.GBM_MAX_DEPTH, criterion='friedman_mse', random_state=experiment_config.MC_RANDOM_SEED)\n",
    "}\n",
    "\n",
    "## K-Fold partition\n",
    "N_SPLITS_CV = experiment_config.CV_NUM_FOLDS # Number of folds for Cross-validation\n",
    "strat_KFold = StratifiedKFold(n_splits=experiment_config.CV_NUM_FOLDS, random_state=experiment_config.MC_RANDOM_SEED, shuffle=True)\n",
    "\n",
    "# Scoring parameters: https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMBINATIONS FOR CROSS-SIMILARITY MATRICES\n",
    "# # Every data representation is going to be assessed with each type of metric\n",
    "# # for the case where the metric is a list, it specifies whether it is for quaternion or other data rep.\n",
    "# combinations_to_analyze = []\n",
    "# for dr, datarep in enumerate(DICT_DATA_REPRESENTATIONS.keys()):\n",
    "#     for mt, motiontransf in enumerate(DICT_MOTION_TRANSFORMATIONS.keys()):\n",
    "#         comb = (datarep,motiontransf) \n",
    "#         combinations_to_analyze.append( comb )\n",
    "#         print(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "Generate paths to write output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IMT/\n"
     ]
    }
   ],
   "source": [
    "STR_DATASET = str(experiment_config.DATASET_MAIN)+\"/\"\n",
    "print(STR_DATASET)\n",
    "def gen_path_plot(filename):\n",
    "    # Generates full paths for PLOTS just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.PLOT_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME, \\\n",
    "                                        file_extension=experiment_config.IMG_FORMAT, save_files=experiment_config.EXPORT_PLOTS)\n",
    "\n",
    "def gen_path_temp(filename, subfolders=\"\", extension=experiment_config.TEMP_FORMAT):\n",
    "    # Generates full paths for TEMP FILES just by specifying a name\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.TEMP_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+subfolders, \\\n",
    "                                        file_extension=extension)\n",
    "\n",
    "def gen_path_results(filename, subfolders=\"\", extension=\"\"):\n",
    "    # Generates full paths for RESULTS FILES (like pandas dataframes)\n",
    "    return utils.generate_complete_path(filename, \\\n",
    "                                        main_folder=experiment_config.RESULTS_FOLDER, \\\n",
    "                                        subfolders=STR_DATASET+NOTEBOOK_SUBFOLDER_NAME+subfolders, \\\n",
    "                                        file_extension=extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASETS: Load and preprocess\n",
    "\n",
    "Datasets are adapted to have the reference right-hand coordinate system used: front=1, left=j, up=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t>>>LOADING DATASETS\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING DATASETS\")\n",
    "dataset = None\n",
    "classes = None\n",
    "\n",
    "# Coordinate reference system. All datasets should be transformed to match this coordinate system.\n",
    "AXIS_INSTANCE=0\n",
    "AXIS_TIME=1\n",
    "AXIS_DIMENSIONS=2\n",
    "\n",
    "# Quaternion representation. All datasets should be transformed to match this quaternion representation.\n",
    "# [qw, qi, qj qk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously processed datasets\n",
    "dataset_quaternion  =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_QUATERNION ) )\n",
    "dataset_euler       =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_EULER ) )\n",
    "dataset_yaw         =   utils.load_binaryfile_npy( gen_path_temp( FILENAME_DATASET_YAW ) )\n",
    "\n",
    "# General variable to extract dataset stats\n",
    "dataset=dataset_quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_config.DATASET_MAIN == Datasets.IMT:\n",
    "    \n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    labels = pd.read_csv(labels_filename)\n",
    "    timestamps = np.loadtxt(timestamps_filename)\n",
    "\n",
    "    # # Classes are the labels of the videos \n",
    "    # classes = labels[\"videoId\"].to_numpy(dtype=np.int32)\n",
    "    # # Classes are the user watching the videos\n",
    "    #classes = labels[\"user\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    classes = labels[experiment_config.CLASS_COLUMN_NAME].to_numpy(dtype=np.int32)\n",
    "\n",
    "if experiment_config.DATASET_MAIN == Datasets.Tsinghua:\n",
    "        \n",
    "    # Data for combined time series to cluster\n",
    "    labels_filename = experiment_config.DATASET_LABELS # Cluster index TRUE_LABEL\n",
    "    timestamps_filename = experiment_config.DATASET_TIMESTAMPS # Timestamps\n",
    "    labels = pd.read_csv(labels_filename)\n",
    "    timestamps = np.loadtxt(timestamps_filename)\n",
    "    \n",
    "    # # Classes are the labels of the videos \n",
    "    # classes = labels[\"videoId\"].to_numpy(dtype=np.int32)\n",
    "    # # Classes are the user watching the videos\n",
    "    #classes = labels[\"user\"].to_numpy(dtype=np.int32)\n",
    "\n",
    "    classes = labels[experiment_config.CLASS_COLUMN_NAME].to_numpy(dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Timestamps: <class 'numpy.ndarray'> (901,)\nDataset <class 'numpy.ndarray'> (290, 901, 4)\nClasses <class 'numpy.ndarray'> (290,)\nnum_classes=5\nnum_ts=290\nlength_ts=901\nnum_dims=4\n"
     ]
    }
   ],
   "source": [
    "num_classes = np.unique(classes).size\n",
    "\n",
    "if(dataset.ndim == 2):\n",
    "    dataset = np.expand_dims(dataset, axis=2)\n",
    "\n",
    "num_ts = dataset.shape[AXIS_INSTANCE]\n",
    "length_ts = dataset.shape[AXIS_TIME]\n",
    "num_dims = dataset.shape[AXIS_DIMENSIONS]\n",
    "\n",
    "print(\"Timestamps:\", type(timestamps), timestamps.shape)\n",
    "print(\"Dataset\", type(dataset), dataset.shape)\n",
    "print(\"Classes\", type(classes), classes.shape)\n",
    "print(f\"num_classes={num_classes}\")\n",
    "print(f\"num_ts={num_ts}\")\n",
    "print(f\"length_ts={length_ts}\")\n",
    "print(f\"num_dims={num_dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Summary`\n",
    "\n",
    "Until this point, the head movements are stored as in these data representations: \n",
    "- Quaternion (`dataset_quaternion`)\n",
    "- Euler (`dataset_euler`)\n",
    "- Yaw (`dataset_yaw`)"
   ]
  },
  {
   "source": [
    "## Differential Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate velocity and acceleration\n",
    "# according to formulas 14-17 in: doi.org/10.1007/s11045-018-0611-3\n",
    " \n",
    "dataset_quat_vel = quaternion_math.quaternion_mult(dataset_quaternion[:,1:,:],quaternion_math.quaternion_conjugate(dataset_quaternion[:,:-1,:]))\n",
    "dataset_quat_acc = quaternion_math.quaternion_mult(dataset_quat_vel[:,1:,:], quaternion_math.quaternion_conjugate(dataset_quat_vel[:,:-1,:]))\n",
    "\n",
    "# dataset_quat_vel = dataset_quaternion[:,1:,:] - dataset_quaternion[:,:-1,:]\n",
    "# dataset_quat_acc = dataset_quat_vel[:,1:,:] - dataset_quat_vel[:,:-1,:]\n",
    "\n",
    "dataset_euler_vel = dataset_euler[:,1:,:] - dataset_euler[:,:-1,:]\n",
    "dataset_euler_acc = dataset_euler_vel[:,1:,:] - dataset_euler_vel[:,:-1,:]\n",
    "\n",
    "dataset_yaw_vel = dataset_yaw[:,1:,:] - dataset_yaw[:,:-1,:]\n",
    "dataset_yaw_acc = dataset_yaw_vel[:,1:,:] - dataset_yaw_vel[:,:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Timestamps: (901,)\nQuat:  (290, 901, 4), (290, 900, 4), (290, 899, 4)\nEuler: (290, 901, 3), (290, 900, 3), (290, 899, 3)\nYaw:   (290, 901, 1), (290, 900, 1), (290, 899, 1)\n>> Resizing!!\nTimestamps: (899,)\nQuat:  (290, 899, 4), (290, 899, 4), (290, 899, 4)\nEuler: (290, 899, 3), (290, 899, 3), (290, 899, 3)\nYaw:   (290, 899, 1), (290, 899, 1), (290, 899, 1)\n"
     ]
    }
   ],
   "source": [
    "# Make all the arrays the same length, and reshape the timestamps accordingly\n",
    "print(f\"Timestamps: {timestamps.shape}\")\n",
    "print(f\"Quat:  {dataset_quaternion.shape}, {dataset_quat_vel.shape}, {dataset_quat_acc.shape}\")\n",
    "print(f\"Euler: {dataset_euler.shape}, {dataset_euler_vel.shape}, {dataset_euler_acc.shape}\")\n",
    "print(f\"Yaw:   {dataset_yaw.shape}, {dataset_yaw_vel.shape}, {dataset_yaw_acc.shape}\")\n",
    "\n",
    "print(f\">> Resizing!!\")\n",
    "diff_timestamps = timestamps[2:]\n",
    "\n",
    "dataset_quaternion = dataset_quaternion[:,2:,:]\n",
    "dataset_quat_vel = dataset_quat_vel[:,1:,:]\n",
    "\n",
    "dataset_euler = dataset_euler[:,2:,:]\n",
    "dataset_euler_vel = dataset_euler_vel[:,1:,:]\n",
    "\n",
    "dataset_yaw = dataset_yaw[:,2:,:]\n",
    "dataset_yaw_vel = dataset_yaw_vel[:,1:,:]\n",
    "\n",
    "print(f\"Timestamps: {diff_timestamps.shape}\")\n",
    "print(f\"Quat:  {dataset_quaternion.shape}, {dataset_quat_vel.shape}, {dataset_quat_acc.shape}\")\n",
    "print(f\"Euler: {dataset_euler.shape}, {dataset_euler_vel.shape}, {dataset_euler_acc.shape}\")\n",
    "print(f\"Yaw:   {dataset_yaw.shape}, {dataset_yaw_vel.shape}, {dataset_yaw_acc.shape}\")"
   ]
  },
  {
   "source": [
    "## Feature extraction\n",
    "\n",
    "Multiple subexperiments:\n",
    "- Quaternion: Raw, Vel, Acc\n",
    "- Euler: Raw, Vel, Acc\n",
    "- Yaw: Raw, Vel, Acc\n",
    "- All: All Quaternion + All Euler (Yaw is included in Euler angles)\n",
    "\n",
    "Each combination with feature-based classifiers:\n",
    "- KNN\n",
    "- DT\n",
    "- RF\n",
    "- GBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t>>>LOADING/CREATING TABULAR FEATURES\n",
      "['./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv']\n",
      "Trying 1/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv']\n",
      "File not found. Creating again! [Errno 2] No such file or directory: './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv'\n",
      "Time breakpoints: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Split indices: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Number of intervals: 120\n",
      "DF features Shape: (432, 120, 60)\n",
      "Time breakpoints: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Split indices: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Number of intervals: 120\n",
      "DF features Shape: (432, 120, 45)\n",
      "Time breakpoints: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Split indices: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Number of intervals: 120\n",
      "DF features Shape: (432, 120, 15)\n",
      "Time breakpoints: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Split indices: [ 36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.  47.  48.  49.\n",
      "  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.\n",
      "  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.\n",
      "  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.\n",
      "  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102. 103. 104. 105.\n",
      " 106. 107. 108. 109. 110. 111. 112. 113. 114. 115. 116. 117. 118. 119.\n",
      " 120. 121. 122. 123. 124. 125. 126. 127. 128. 129. 130. 131. 132. 133.\n",
      " 134. 135. 136. 137. 138. 139. 140. 141. 142. 143. 144. 145. 146. 147.\n",
      " 148. 149. 150. 151. 152. 153. 154.]\n",
      "Number of intervals: 120\n",
      "DF features Shape: (432, 120, 105)\n",
      "Trying 2/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv', './results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv']\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_quat.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_euler.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_yaw.csv was successfully loaded\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/datasets_feature_based/data_features_all.csv was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING/CREATING TABULAR FEATURES\")\n",
    "# Filename of the file containing demographics and HMD movements data\n",
    "features_quaternion_filename = gen_path_results(\"data_features_quat\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_euler_filename      = gen_path_results(\"data_features_euler\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_yaw_filename        = gen_path_results(\"data_features_yaw\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "features_all_filename        = gen_path_results(\"data_features_all\", subfolders=\"datasets_feature_based/\", extension=\".csv\")\n",
    "\n",
    "# Placeholders\n",
    "features_quat  = None\n",
    "features_euler = None\n",
    "features_yaw   = None\n",
    "features_all   = None\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [features_quaternion_filename, features_euler_filename, features_yaw_filename, features_all_filename]\n",
    "\n",
    "print(input_files)\n",
    "\n",
    "RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        features_quat = pd.read_csv(input_files[0]) # pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "        features_euler = pd.read_csv(input_files[1]) # pd.DataFrame\n",
    "        print(f\"File {input_files[1]} was successfully loaded\")\n",
    "        features_yaw = pd.read_csv(input_files[2]) # pd.DataFrame\n",
    "        print(f\"File {input_files[2]} was successfully loaded\")\n",
    "        features_all = pd.read_csv(input_files[3]) # pd.DataFrame\n",
    "        print(f\"File {input_files[3]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        # Combine time series to extract joint features\n",
    "        ARRAYS = [  np.concatenate([dataset_quaternion, dataset_quat_vel, dataset_quat_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_euler, dataset_euler_vel, dataset_euler_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_yaw, dataset_yaw_vel, dataset_yaw_acc], axis=AXIS_DIMENSIONS), \\\n",
    "                    np.concatenate([dataset_quaternion, dataset_quat_vel, dataset_quat_acc,                      \n",
    "                                        dataset_euler, dataset_euler_vel, dataset_euler_acc], axis=AXIS_DIMENSIONS)\n",
    "                 ]\n",
    "\n",
    "        HEADERS = [ HEADER_QUAT, HEADER_EULER, HEADER_YAW, HEADER_ALL]\n",
    "\n",
    "        for idx in range(len(ARRAYS)):\n",
    "            array_to_process = ARRAYS[idx]\n",
    "            header_to_process = HEADERS[idx]\n",
    "            data_feat_array, colnames = ts_processing.extract_summary_statistics_with_non_overlapping_time_window(array_to_process, \\\n",
    "                                                                diff_timestamps, \\\n",
    "                                                                class_labels=classes, \\\n",
    "                                                                time_limit_secs=(timestamps[0],timestamps[-1]), \\\n",
    "                                                                window_width_secs=SLIDING_WINDOW_WIDTH_SECS, \\\n",
    "                                                                dim_names = header_to_process)\n",
    "            dataframe_features = pd.DataFrame(data=data_feat_array, columns=colnames)\n",
    "            for i in [1, 2, -1]:\n",
    "                dataframe_features[colnames[i]] = dataframe_features[colnames[i]].astype(int)\n",
    "            dataframe_features.to_csv(input_files[idx], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `NOTE:` REDEFINITION OF DATASETS\n",
    "The dictionary is redefined now, since the datasets were properly loaded/created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset 'quaternion' shape: \t(51840, 64)\nDataset 'euler' shape: \t(51840, 49)\nDataset 'yaw' shape: \t(51840, 19)\nDataset 'all' shape: \t(51840, 109)\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = num_classes # Global variable counting number of different classes in the dataset\n",
    "# REDEFINE DICT WITH CORRESPONDING DATASETS\n",
    "DICT_DATA = {\n",
    "    DataRepresentation.Quaternion:  features_quat.copy(),\n",
    "    DataRepresentation.Euler:       features_euler.copy(),\n",
    "    DataRepresentation.Yaw:         features_yaw.copy(),\n",
    "    DataRepresentation.All:         features_all.copy()\n",
    "}\n",
    "\n",
    "for dr, _data in DICT_DATA.items():\n",
    "    print(f\"Dataset '{dr}' shape: \\t{_data.shape}\")"
   ]
  },
  {
   "source": [
    "---\n",
    "## Feature-based Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains the dataframe with compiled results\n",
    "classif_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t>>>LOADING/CREATING CLASSIFICATION RESULTS\n",
      "Trying 1/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv']\n",
      "File not found. Creating again! [Errno 2] No such file or directory: './results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv'\n",
      "Iterating 16 times\n",
      " | Iteration 0/16 \t> dataset: quaternion \tclassifier: Classifiers.KNN\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  1.9min remaining:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  3.1min remaining:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  3.2min remaining:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.3min finished\n",
      " | Iteration 1/16 \t> dataset: quaternion \tclassifier: Classifiers.DT\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:   21.5s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:   22.1s remaining:   18.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:   40.4s remaining:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   40.6s finished\n",
      " | Iteration 2/16 \t> dataset: quaternion \tclassifier: Classifiers.RF\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  2.1min remaining:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  2.1min remaining:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  4.0min remaining:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  4.0min finished\n",
      " | Iteration 3/16 \t> dataset: quaternion \tclassifier: Classifiers.GBM\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed: 20.8min remaining: 83.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed: 21.0min remaining: 17.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed: 36.4min remaining:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 36.4min finished\n",
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 4/16 \t> dataset: euler \tclassifier: Classifiers.KNN\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:   55.6s remaining:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  1.8min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  1.9min remaining:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.9min finished\n",
      " | Iteration 5/16 \t> dataset: euler \tclassifier: Classifiers.DT\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:   17.6s remaining:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:   19.0s remaining:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:   34.3s remaining:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   34.5s finished\n",
      " | Iteration 6/16 \t> dataset: euler \tclassifier: Classifiers.RF\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  2.0min remaining:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  2.0min remaining:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  3.5min remaining:   23.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  3.5min finished\n",
      " | Iteration 7/16 \t> dataset: euler \tclassifier: Classifiers.GBM\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed: 18.4min remaining: 73.7min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed: 18.6min remaining: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed: 32.7min remaining:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 32.7min finished\n",
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 8/16 \t> dataset: yaw \tclassifier: Classifiers.KNN\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  1.4min remaining:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  1.9min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  2.1min remaining:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.1min finished\n",
      " | Iteration 9/16 \t> dataset: yaw \tclassifier: Classifiers.DT\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:    6.4s remaining:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:    6.7s remaining:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:   11.9s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   12.0s finished\n",
      " | Iteration 10/16 \t> dataset: yaw \tclassifier: Classifiers.RF\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  1.3min remaining:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  1.4min remaining:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  2.4min remaining:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  2.4min finished\n",
      " | Iteration 11/16 \t> dataset: yaw \tclassifier: Classifiers.GBM\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  6.4min remaining: 25.7min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  6.6min remaining:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed: 11.9min remaining:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 11.9min finished\n",
      "\t\t>> FINISHED (dataset,distMetr)\n",
      " | Iteration 12/16 \t> dataset: all \tclassifier: Classifiers.KNN\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  1.0min remaining:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  1.7min remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  1.7min remaining:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.8min finished\n",
      " | Iteration 13/16 \t> dataset: all \tclassifier: Classifiers.DT\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:   43.7s remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:   44.5s remaining:   36.4s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  1.3min remaining:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.3min finished\n",
      " | Iteration 14/16 \t> dataset: all \tclassifier: Classifiers.RF\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed:  3.5min remaining: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed:  3.6min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  6.1min remaining:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  6.1min finished\n",
      " | Iteration 15/16 \t> dataset: all \tclassifier: Classifiers.GBM\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  20 | elapsed: 37.2min remaining: 148.6min\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  20 | elapsed: 37.6min remaining: 30.8min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed: 67.6min remaining:  7.5min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed: 67.8min finished\n",
      "\t\t>> FINISHED (dataset,distMetr)\n",
      "Trying 2/2 to load files: ['./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv']\n",
      "File ./results/Tsinghua/2_FeatureBasedClassifiers/classif_results.csv was successfully loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t>>>LOADING/CREATING CLASSIFICATION RESULTS\")\n",
    "# Filename of the file containing demographics and HMD movements data\n",
    "classification_results_filename = gen_path_results(experiment_config.RESULTS_FILENAME, extension=\".csv\")\n",
    "\n",
    "### INPUTS / OUTPUTS\n",
    "\"\"\"EDIT CUSTOM FILENAMES\"\"\"\n",
    "input_files = [classification_results_filename]\n",
    "\n",
    "RELOAD_TRIES = experiment_config.RELOAD_TRIES\n",
    "# Try to load files maximum two times\n",
    "for tries in range(RELOAD_TRIES):\n",
    "    try:\n",
    "        ### LOAD FILE\n",
    "        print(f\"Trying {tries+1}/{RELOAD_TRIES} to load files: {input_files}\")\n",
    "        \n",
    "        ### CUSTOM SECTION TO READ FILES\n",
    "        \"\"\"EDIT CUSTOM READ\"\"\"\n",
    "        classif_results = pd.read_csv(input_files[0]) # pd.DataFrame\n",
    "        print(f\"File {input_files[0]} was successfully loaded\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        ### CREATE FILE\n",
    "        print(f\"File not found. Creating again! {e}\")\n",
    "\n",
    "        ### CUSTOM SECTION TO CREATE FILES \n",
    "        \"\"\"EDIT CUSTOM WRITE\"\"\"\n",
    "        \n",
    "        idx = 1 # Iterator for row index\n",
    "        total_iter = len(DICT_DATA.keys()) * len(DICT_CLASSIFIERS.keys()) # How many total iterations are going to be conducted\n",
    "        print(f\"Iterating {total_iter} times\")\n",
    "        # f = IntProgress(min=0, max=classif_results.shape[0])\n",
    "        # display(f)\n",
    "\n",
    "        ### Iterate over datasets and classifiers\n",
    "\n",
    "        # For each dataset\n",
    "        for iter_datarep, iter_data in DICT_DATA.items():\n",
    "            # Apply each classifier model\n",
    "            for iter_classifier,iter_model in DICT_CLASSIFIERS.items():\n",
    "                \n",
    "                # Print messages\n",
    "                if idx%experiment_config.DISPLAY_ITER_STEP==0: \n",
    "                    print(f\" | Iteration {idx}/{total_iter} \\t> dataset: {str(iter_datarep)} \\tclassifier: {str(iter_classifier)}\")\n",
    "                idx = idx + 1\n",
    "\n",
    "                # Separate training and testing sets\n",
    "                iter_datadata = iter_data.drop(['instanceId','timeId','timestamp'], axis='columns')\n",
    "                X = iter_data.drop([\"class\"],axis=\"columns\")\n",
    "                y = iter_data[\"class\"]\n",
    "\n",
    "                # Standardize (z-norm) if model is KNN\n",
    "                if iter_model is DICT_CLASSIFIERS[Classifiers.KNN]:\n",
    "                    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                # Cross-validation (CV)\n",
    "                cv_results =  cross_validate(iter_model, X, y, cv=strat_KFold, scoring=SCORING_METRICS, n_jobs=experiment_config.N_JOBS_PARALLEL, verbose=3) # \n",
    "\n",
    "                # Add more information to CV results\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[0]] = [ str(iter_datarep) ] * N_SPLITS_CV # Datarep\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[1]] = [ str(iter_classifier) ] * N_SPLITS_CV # Classifier\n",
    "                cv_results[experiment_config.COLUMNS_LABELS[2]] = np.arange(N_SPLITS_CV) # Fold\n",
    "                # for k,v in cv_results.items():\n",
    "                    # print(f\"{k}: \\t{v}\")\n",
    "\n",
    "                # From dict to pandas DataFrame\n",
    "                iteration_results = pd.DataFrame(cv_results.copy(), columns=sorted(cv_results.keys()))\n",
    "\n",
    "                # Extend dataframe\n",
    "                if classif_results is None:\n",
    "                    classif_results = iteration_results\n",
    "                else:\n",
    "                    classif_results = classif_results.append(iteration_results, ignore_index=True)\n",
    "\n",
    "                # END: Classifiers\n",
    "\n",
    "            # } END: Dataset\n",
    "            print(f\"\\t\\t>> FINISHED (dataset,distMetr)\")\n",
    "        # } END: Loop done\n",
    "\n",
    "        # for i in [0,1,2,3]:\n",
    "            # classif_results[experiment_config.COLUMNS_LABELS[i]] = classif_results[experiment_config.COLUMNS_LABELS[i]].astype(int)\n",
    "\n",
    "        # Save files\n",
    "        classif_results.to_csv(input_files[0], index=False)\n",
    "\n",
    "        ### ---- CONTROL RETRIES\n",
    "        if tries+1 < RELOAD_TRIES:\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    break"
   ]
  },
  {
   "source": [
    "---\n",
    "## Plot"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for metric_txt in DICT_DIST_METRICS.keys():\n",
    "\n",
    "#     ## Data preprocessing for plotting\n",
    "#     # Take only accuracy for class1 over all runs and delete irrelevant columns\n",
    "#     plotsdataset = classif_results[( (classif_results['classLabel']==1) & (classif_results['distMetric'] == DICT_DISTMETRIC_TO_NUM[metric_txt]) )]   # Filter for plotting\n",
    "#     plotsdataset.drop(['mcIter','classLabel','precision','recall'], axis=1, inplace=True)\n",
    "#     plotsdataset.tail()\n",
    "\n",
    "#     ## Plot\n",
    "#     if(experiment_config.SHOW_PLOTS): plot_violin_mc(plotsdataset, x_colname=\"dataRep\", y_colname=\"accuracy\", hue_colname=\"classifier\",\\\n",
    "#                     suptitle=f'Classification accuracy over {MC_ITERATIONS} Monte-Carlo simulations',\\\n",
    "#                     title = f\"Distance Measure = {metric_txt}\",\\\n",
    "#                     x_ticklabels = experiment_config.HEADERS_DATASETS, \\\n",
    "#                     y_lim=[0,1], \\\n",
    "#                     n_rows=1, n_cols=1, figsize=(8,6), \\\n",
    "#                     save_path=gen_path_plot(f\"accuracies_{metric_txt}\"),\n",
    "#                     boxplot_instead_violin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> FINISHED WITHOUT ERRORS!!\n"
     ]
    }
   ],
   "source": [
    "print(\">> FINISHED WITHOUT ERRORS!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "f3aec1f4fef7a88c2258d5b84a8b82909f076cff2bcb16988c856ebc42b66954"
   }
  },
  "interpreter": {
   "hash": "b9fb952a9e6e460b8e2319dbdd5bbe40cc785f6679ade5cf0077f2ef42b5e713"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}